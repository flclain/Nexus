hydra:
  run:
    dir: ${output_dir}
  output_subdir: ${output_dir}/code/hydra           # Store hydra's config breakdown here for debugging
  searchpath:                                       # Only <exp_dir> in these paths are discoverable
    - pkg://nuplan.planning.script.config.common
    - pkg://nuplan.planning.script.config.training  # Put experiments configs in script/experiments/<exp_dir>
    - pkg://nuplan_extent.planning.script.experiments
    - pkg://nuplan_extent.planning.script.config.training
    - pkg://nuplan_extent.planning.script.config.common
    - pkg://nuplan_extent.planning.script.config.common_cfg
    - pkg://nuplan_extent.planning.script.config.splitter

defaults:
  - default_experiment
  - default_common

  # Trainer and callbacks
  - lightning: default_lightning
  - callbacks: default_callbacks

  # Optimizer settings
  - optimizer: adam # [adam, adamw] supported optimizers
  - lr_scheduler: null # [one_cycle_lr] supported lr_schedulers
  - warm_up_lr_scheduler: null # [linear_warm_up, constant_warm_up] supported warm up lr schedulers

  # Data Loading
  - data_loader: default_data_loader
  - splitter: ???

  # Objectives and metrics
  - objective: ???
  - training_metric: ???
  - aggregated_metric: null
  - data_augmentation: null
  - val_data_augmentation: null
  - data_augmentation_scheduler: null  # [default_augmentation_schedulers, stepwise_augmentation_probability_scheduler, stepwise_noise_parameter_scheduler] supported data augmentation schedulers
  - scenario_type_weights: default_scenario_type_weights

  # Closed-loop controller to be used. Will activate if closed_loop=true
  - ego_controller: null

  # common cfg that used across multiple configs
  - common_cfg: null

experiment_name: 'training'
closed_loop: false

# Cache parameters
cache:
  cache_path: ${oc.env:NUPLAN_EXP_ROOT}/cache         # Local/remote path to store all preprocessed artifacts from the data pipeline
  use_cache_without_dataset: True                    # Load all existing features from a local/remote cache without loading the dataset
  force_feature_computation: false                    # Recompute features even if a cache exists
  cleanup_cache: false                                # Cleanup cached data in the cache_path, this ensures that new data are generated if the same cache_path is passed
  cache_metadata_path: null
  force_recompute_features: null
  versatile_caching: false

# Mandatory parameters
py_func: ???                                          # Function to be run inside main (can be "train", "test", "cache")

# Pre-train checkpoint
checkpoint: null                                     # Specify your training pre-train checkpoint here
